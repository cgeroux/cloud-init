#cloud-config
apt_sources:
  - source: "ppa:saltstack/salt"
packages:
  - python-software-properties
  - git
  - salt-master
  - salt-minion
  - make
write_files:
  - content: |
        roles:
           - hadoop_master
           - spark_target
        hdfs_data_disks:
           - /mnt
    path: /etc/salt/grains
  - content: |
        #!/bin/bash
        sed -i 's/^#auto_accept.*$/auto_accept: True/g' /etc/salt/master
        sudo service salt-master restart
    path: /tmp/bootstrap-master.sh
    permissions: "0755"
  - content: |
        #!/bin/bash
        sed -i 's/^#master.*$/master: 127.0.0.1/g' /etc/salt/minion
        sudo service salt-minion restart
    path: /tmp/bootstrap-minion.sh
    permissions: "0755"
  - content: |
      file_roots:
        base:
          - /srv/salt
          - /srv/salt/formulas/hadoop-formula
          - /srv/salt/formulas/hostsfile-formula
          - /srv/salt/formulas/sun-java-formula
    path: /etc/salt/master.d/file_roots.conf
    permissions: '0644'
  - content: |
      pillar_roots:
        base:
          - /srv/pillar
    path: /etc/salt/master.d/pillar_roots.conf
  - content: |
      base:
        'G@roles:hadoop_slave or G@roles:hadoop_master':
          - match: compound
          - hostsfile
          - hostsfile.hostname
          - sun-java
          - sun-java.env
          - hadoop
          - hadoop.hdfs
          - hadoop.mapred
          - hadoop.yarn
          - hadoop.spark
    path: /srv/salt/top.sls
  - content: |
      base:
        'G@roles:hadoop_slave or G@roles:hadoop_master':
           - hadoop
    path: /srv/pillar/top.sls
  - content: |
      spark:
        version: 1.6.1-bin-hadoop2.6
        spark_target: "G@roles:hadoop_slave or G@roles:hadoop_master"
        spark_master: "G@roles:hadoop_master"
        spark_slave: "G@roles:hadoop_slave"
        targeting_method: compound
      hadoop:
        version: apache-2.7.1 # ['apache-1.2.1', 'apache-2.2.0', 'hdp-1.3.0', 'hdp-2.2.0', 'cdh-4.5.0', 'cdh-4.5.0-mr1']
        targeting_method: grain # [compound, glob] also supported
        users:
          hadoop: 6000
          hdfs: 6001
          mapred: 6002
          yarn: 6003
        config:
          directory: /etc/hadoop/conf
          core-site:
            io.native.lib.available:
              value: true
            io.file.buffer.size:
              value: 65536
            fs.trash.interval:
              value: 60
      hdfs:
        namenode_target: "roles:hadoop_master" # Specify compound matching string to match all your namenodes
        datanode_target: "roles:hadoop_slave" # Specify compound matching string to match all your datanodes e.g. if you were to use pillar I@datanode:true
        config:
          namenode_port: 8020
          namenode_http_port: 50070
          secondarynamenode_http_port: 50090
          # the number of hdfs replicas is normally auto-configured for you in hdfs.settings
          # according to the number of available datanodes
          # replication: 1
          hdfs-site:
            dfs.permission:
              value: false
            dfs.durable.sync:
              value: true
            dfs.datanode.synconclose:
              value: true
      mapred:
        jobtracker_target: "roles:hadoop_master"
        tasktracker_target: "roles:hadoop_slave"
        config:
          jobtracker_port: 9001
          jobtracker_http_port: 50030
          jobhistory_port: 10020
          jobhistory_webapp_port: 19888
          history_dir: /mr-history
          mapred-site:
            mapred.map.memory.mb:
              value: 1536
            mapred.map.java.opts:
              value: -Xmx1024M
            mapred.reduce.memory.mb:
              value: 3072
            mapred.reduce.java.opts:
              value: -Xmx1024m
            mapred.task.io.sort.mb:
              value: 512
            mapred.task.io.sort.factor:
              value: 100
            mapred.reduce.shuffle.parallelcopies:
              value: 50
            mapreduce.framework.name:
              value: yarn
      yarn:
        resourcemanager_target: "roles:hadoop_master"
        nodemanager_target: "roles:hadoop_slave"
        config:
          yarn-site:
            yarn.nodemanager.aux-services:
              value: mapreduce_shuffle
            yarn.nodemanager.aux-services.mapreduce.shuffle.class:
              value: org.apache.hadoop.mapred.ShuffleHandler
            yarn.resourcemanager.scheduler.class:
              value: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
          capacity-scheduler:
            yarn.scheduler.capacity.maximum-applications:
              value: 10000
            yarn.scheduler.capacity.resource-calculator:
              value: org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
            yarn.scheduler.capacity.root.queues:
              value: default,customqueue
            yarn.scheduler.capacity.root.capacity:
              value: 100
            yarn.scheduler.capacity.root.default.capacity:
              value: 70
            yarn.scheduler.capacity.root.default.user-limit-factor:
              value: 1
            yarn.scheduler.capacity.root.default.maximum-capacity:
              value: 100
            yarn.scheduler.capacity.root.default.state:
              value: RUNNING
            yarn.scheduler.capacity.root.default.acl_submit_applications:
              value: '*'
            yarn.scheduler.capacity.root.default.acl_administer_queue:
              value: '*'
            yarn.scheduler.capacity.root.customqueue.capacity:
              value: 30
            yarn.scheduler.capacity.root.customqueue.maximum-am-resource-percent:
              value: 1.0
            yarn.scheduler.capacity.root.customqueue.user-limit-factor:
              value: 1
            yarn.scheduler.capacity.root.customqueue.maximum-capacity:
              value: 100
            yarn.scheduler.capacity.root.customqueue.state:
              value: RUNNING
            yarn.scheduler.capacity.root.customqueue.acl_submit_applications:
              value: '*'
            yarn.scheduler.capacity.root.customqueue.acl_administer_queue:
              value: '*'
            yarn.scheduler.capacity.node-locality-delay:
              value: -1
    path: /srv/pillar/hadoop.sls
  - content: |
        mine_functions:
            network.interfaces: []
            network.ip_addrs: []
            grains.items: []
    path: /etc/salt/minion.d/mine_functions.conf

runcmd:
  - bash /tmp/bootstrap-master.sh
  - bash /tmp/bootstrap-minion.sh
  - git clone https://github.com/cgeroux/hadoop-formula.git /srv/salt/formulas/hadoop-formula
  - git clone https://github.com/cgeroux/hostsfile-formula.git /srv/salt/formulas/hostsfile-formula
  - git clone https://github.com/cgeroux/sun-java-formula.git /srv/salt/formulas/sun-java-formula
  - git clone https://github.com/cgeroux/init-salt.git /tmp/salt-init
  - sudo service salt-master restart
  - cd /srv/salt/formulas/hadoop-formula/hadoop/files;./generate_keypairs.sh
  - /tmp/salt-init/salt-init.py MINIONLIST
  - sudo -H -u spark bash -c "/usr/lib/spark/sbin/start-all.sh"
